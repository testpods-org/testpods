# Flow CLI Learnings System

This guide covers the learnings and self-improvement system for Flow CLI. The learnings system captures execution insights and improves agent behavior over time through both automated analysis and human feedback.

## Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
- [Core Concepts](#core-concepts)
- [How-To Guides](#how-to-guides)
  - [Setting Up the Learnings System](#setting-up-the-learnings-system)
  - [Submitting Human Feedback](#submitting-human-feedback)
  - [Viewing Learnings](#viewing-learnings)
  - [Running Automated Analysis](#running-automated-analysis)
  - [Pattern Detection with Aggregator](#pattern-detection-with-aggregator)
  - [Integrating with Orchestration Scripts](#integrating-with-orchestration-scripts)
- [Architecture](#architecture)
- [Command Reference](#command-reference)
- [Customization](#customization)

## Overview

The learnings system enables continuous improvement of your agent workflows through:

1. **Human Feedback** - Submit findings from testing, code review, and bug discovery
2. **Automated Analysis** - Agent-based analysis of completed flows
3. **Pattern Detection** - Cross-flow aggregation to identify recurring issues
4. **Improvement PRs** - Automated proposals for agent instruction improvements

### Key Benefits

- **Learn from every execution** - Both successes and failures become learning opportunities
- **Capture human insights** - Smoke tests, code reviews, and bug reports are preserved
- **Identify patterns** - Recurring issues across flows surface automatically
- **Review before applying** - All improvements go through PR review, not direct edits

## Quick Start

### 1. Scaffold the Learnings System

```bash
flow learnings scaffold
```

This creates configuration files, agent prompts, and orchestration scripts in your project.

### 2. Submit Feedback After Testing

After testing a flow's output, submit your findings:

```bash
# Bug found during smoke testing
flow feedback --flow impl-auth --category smoke_test --severity high \
  --title "Login fails silently" \
  --description "No error message when password is wrong"

# Finding from code review
flow feedback --flow impl-auth --category code_review --severity medium \
  --title "Missing input validation" \
  --file src/auth/login.py --tag validation
```

### 3. View Accumulated Learnings

```bash
# View recent learnings
flow learnings --flow-type implement

# View human feedback only
flow learnings --flow-type implement --source human_feedback

# View pattern clusters
flow learnings --flow-type implement --clusters
```

### 4. Run Analysis After Flow Completion

```bash
# Run the learnings analyst
./.flow/scripts/learnings/run_analyst.py impl-auth
```

The analyst will analyze the flow's execution artifacts and create a PR with proposed improvements.

## Core Concepts

### Learnings Database

Learnings are stored per flow-type in `.flow/{flow_type}/learnings.jsonl`:

```
.flow/
  implement/
    learnings.jsonl           # Learnings for implementation flows
    learning_clusters.json    # Pattern clusters
  dependency-update/
    learnings.jsonl           # Learnings for dependency update flows
```

### Learning Entry Structure

Each learning entry contains:

| Field           | Description                                                          |
| --------------- | -------------------------------------------------------------------- |
| `id`            | Unique identifier (UUID)                                             |
| `timestamp`     | When the entry was created                                           |
| `source`        | flow_analysis, human_feedback, or aggregation                        |
| `category`      | pattern, anti_pattern, tooling, instruction, bug, missed_requirement |
| `title`         | Brief descriptive title                                              |
| `description`   | Detailed explanation                                                 |
| `flow_type`     | The flow type (e.g., "implement")                                    |
| `flow_name`     | Specific flow name (optional)                                        |
| `step`          | Step number (optional)                                               |
| `related_files` | Files related to this learning                                       |
| `severity`      | For feedback: low, medium, high, critical                            |
| `tags`          | Tags for clustering                                                  |

### Sources

- **flow_analysis** - Generated by the learnings-analyst agent after flow completion
- **human_feedback** - Submitted via `flow feedback` command
- **aggregation** - Generated by the learnings-aggregator when detecting patterns

### Categories

| Category             | Description                     | Example                           |
| -------------------- | ------------------------------- | --------------------------------- |
| `pattern`            | Effective pattern to reinforce  | "Using type hints everywhere"     |
| `anti_pattern`       | Pattern to avoid                | "Catching bare exceptions"        |
| `tooling`            | Tool or process improvement     | "Add pre-commit hook for linting" |
| `instruction`        | Agent instruction update needed | "Clarify error handling in docs"  |
| `bug`                | Bug discovered                  | "Null pointer in edge case"       |
| `missed_requirement` | Requirement not implemented     | "Forgot pagination support"       |

## How-To Guides

### Setting Up the Learnings System

#### Step 1: Scaffold the Configuration

```bash
# Preview what will be created
flow learnings scaffold --dry-run

# Create the learnings system
flow learnings scaffold
```

This creates:

```
.flow/
  learnings/
    analyst.yaml              # Flow config for learnings-analyst
    aggregator.yaml           # Flow config for learnings-aggregator
    agents/
      learnings-analyst.md    # Analyst agent prompt
      learnings-aggregator.md # Aggregator agent prompt
  scripts/
    learnings/
      run_analyst.py          # Run after flow completion
      run_aggregator.py       # Run for pattern detection
```

#### Step 2: Review and Customize Agent Prompts

The agent prompts in `.flow/learnings/agents/` are fully customizable. Review them and adjust for your project's:

- Coding standards
- Architecture patterns
- Review criteria
- Domain-specific concerns

#### Step 3: Verify Configuration

Check that the flow configs reference valid base configuration:

```yaml
# .flow/learnings/analyst.yaml
base_config: .flow/base.yaml # Adjust if your base config is elsewhere
flow_type: learnings
```

### Submitting Human Feedback

#### After Smoke Testing

When you test the implementation and find issues:

```bash
# Critical bug
flow feedback --flow impl-user-crud --category smoke_test --severity critical \
  --title "Create user crashes server" \
  --description "POST /users returns 500 with valid payload"

# Minor UI issue
flow feedback --flow impl-user-crud --category smoke_test --severity low \
  --title "Button misaligned" \
  --description "Submit button shifts 2px on hover" \
  --file src/components/SubmitButton.tsx
```

#### After Code Review

When reviewing agent-generated code:

```bash
# Security concern
flow feedback --flow impl-auth --category code_review --severity high \
  --title "Password logged in plaintext" \
  --file src/auth/login.py \
  --tag security --tag logging

# Style issue
flow feedback --flow impl-auth --category code_review --severity low \
  --title "Inconsistent naming" \
  --description "Mix of camelCase and snake_case in auth module"
```

#### With Detailed Description from File

For complex feedback, write the description in a file:

```bash
# Create detailed description
cat > feedback-details.md << 'EOF'
## Issue

The authentication flow doesn't handle session expiry correctly.

## Steps to Reproduce

1. Login successfully
2. Wait 30 minutes
3. Try to access protected route
4. Observe: Returns 500 instead of redirecting to login

## Expected Behavior

Should redirect to /login with message "Session expired"
EOF

# Submit with file reference
flow feedback --flow impl-auth --category bug --severity high \
  --title "Session expiry not handled" \
  --description @feedback-details.md
```

#### Using Tags for Pattern Detection

Tags help the aggregator identify patterns across flows:

```bash
# Tagging helps identify recurring issues
flow feedback --flow impl-api --category bug --title "Race condition in cache" \
  --tag concurrency --tag cache

flow feedback --flow impl-user --category bug --title "Race condition in login" \
  --tag concurrency --tag auth

# Later, aggregator can find: "3+ concurrency issues detected"
```

### Viewing Learnings

#### Basic Queries

```bash
# All recent learnings for a flow type
flow learnings --flow-type implement

# Limit results
flow learnings --flow-type implement --limit 5

# Specific flow
flow learnings --flow impl-auth
```

#### Filtering by Source

```bash
# Human feedback only
flow learnings --flow-type implement --source human_feedback

# Agent analysis only
flow learnings --flow-type implement --source flow_analysis

# Pattern aggregations
flow learnings --flow-type implement --source aggregation
```

#### Filtering by Category

```bash
# View anti-patterns to avoid
flow learnings --flow-type implement --category anti_pattern

# View bugs to address
flow learnings --flow-type implement --category bug

# View missed requirements
flow learnings --flow-type implement --category missed_requirement
```

#### Viewing Pattern Clusters

When the aggregator has identified patterns:

```bash
# Show clusters instead of individual entries
flow learnings --flow-type implement --clusters
```

Output:

```
Learning Clusters for 'implement' (3 clusters)

┌────────────────────────┬─────────────┬─────────────────────┬────────────────────────────────┬─────────────────────┐
│ Title                  │ Occurrences │ Sources             │ Description                    │ Updated             │
├────────────────────────┼─────────────┼─────────────────────┼────────────────────────────────┼─────────────────────┤
│ Pattern: validation    │ 5           │ human_feedback      │ Cluster of 5 learnings tagged  │ 2025-01-15T14:30:00 │
│ Recurring anti patterns│ 3           │ flow_analysis       │ Cluster of 3 anti_pattern...   │ 2025-01-15T13:00:00 │
│ Issues with login.py   │ 4           │ human_feedback, ... │ Cluster of 4 learnings...      │ 2025-01-15T12:00:00 │
└────────────────────────┴─────────────┴─────────────────────┴────────────────────────────────┴─────────────────────┘
```

#### Cross-Flow-Type Queries

```bash
# View all learnings across all flow types
flow learnings --all-types

# Filter across all types
flow learnings --all-types --category bug
```

#### JSON Output for Scripting

```bash
# Get JSON for programmatic processing
flow learnings --flow-type implement --json | jq '.entries[].title'
```

### Running Automated Analysis

#### After Flow Completion

The learnings-analyst examines completed flow artifacts and identifies:

- What worked well (patterns)
- What didn't work (anti-patterns)
- Bugs discovered during execution
- Missed requirements from the plan

```bash
# Run analysis after a flow completes
./.flow/scripts/learnings/run_analyst.py impl-auth
```

The analyst:

1. Reads flow artifacts via `flow artifacts --flow impl-auth`
2. Analyzes implementation log, step reports, and events
3. Writes entries to the learnings database
4. Creates a PR with proposed improvements to agent instructions

#### What the Analyst Examines

- **Flow artifacts** - Timing, steps, agent status, retries
- **Agent reports** - Success/failure summaries
- **Implementation log** - Decisions and patterns across steps
- **Git commits** - What changes were made per step

#### Reviewing Analyst PRs

After analysis, check for a new PR:

```bash
gh pr list --label learnings
```

Review the proposed changes carefully:

- Are the improvements based on solid evidence?
- Do they align with your project's standards?
- Could they have unintended side effects?

### Pattern Detection with Aggregator

The aggregator identifies recurring patterns across multiple flows:

```bash
# Analyze patterns for a specific flow type
./.flow/scripts/learnings/run_aggregator.py implement

# Analyze patterns across all flow types
./.flow/scripts/learnings/run_aggregator.py --all-types
```

#### When to Run the Aggregator

- After accumulating learnings from 5+ flows
- Periodically (weekly/monthly) for mature projects
- Before major refactoring to understand common issues

#### How Patterns Are Detected

The aggregator groups learnings by:

1. **Tags** - Common themes like "validation", "auth", "security"
2. **Related files** - Same files appearing in multiple issues
3. **Categories** - Recurring anti-patterns or bugs

Clusters are created when 3+ entries share a grouping criterion.

### Integrating with Orchestration Scripts

#### Separated Workflow Scripts

The implementation workflow is separated into standalone scripts for granular control:

| Script                                             | Purpose                                          |
| -------------------------------------------------- | ------------------------------------------------ |
| `.flow/scripts/flows/implement.py`                 | Implementation only (build/review/commit cycles) |
| `.flow/scripts/flows/run_learnings.py`             | Standalone learnings analysis (idempotent)       |
| `.flow/scripts/flows/run_proof.py`                 | Standalone proof generation (idempotent)         |
| `.flow/scripts/flows/implement_learnings_proof.py` | Orchestrator running all three                   |

**Recommended usage:**

```bash
# Full workflow - runs implement, then learnings+proof in parallel
./.flow/scripts/flows/implement_learnings_proof.py specs/my-feature.md

# Or run components separately
./.flow/scripts/flows/implement.py specs/my-feature.md
./.flow/scripts/flows/run_learnings.py --flow-dir flows/implement_my-feature --plan specs/my-feature.md
```

**Workflow diagram:**

```
┌─────────────────────────────────────────────────────┐
│  implement.py: Build/Review/Commit Cycles            │
│  - Documentation Update (commit)                     │
└─────────────────────────────────────────────────────┘
                        │
                        ▼
        ┌───────────────┴───────────────┐
        │                               │
        ▼                               ▼
┌───────────────────┐     ┌───────────────────────────┐
│  learnings.py     │     │  proof.py                  │
│  (parallel)       │     │  (parallel)                │
│  - Analysis flow  │     │  - Evidence collection     │
│  - Review guide   │     │  - Narrative generation    │
└───────────────────┘     └───────────────────────────┘
```

**Idempotent behavior:**

All scripts are idempotent - safe to re-run:

- `learnings.py`: Skips if `learnings-{flow_name}` already completed
- `proof.py`: Skips if `proof-{flow_name}` already completed, or if plan has no showcase section

**Retry failed components:**

If learnings or proof fails, retry commands are shown:

```
=== Partial Success ===
Implementation completed but the following post-tasks failed:
  - proof

Retry commands:
  ./.flow/scripts/flows/run_proof.py --flow-dir flows/implement_my-feature --plan specs/my-feature.md
```

**Example output (full workflow):**

```
=== Running Implementation ===
[...implementation output...]

=== Post-Implementation Tasks ===
Running: learnings, proof
  OK learnings
  OK proof

=== Complete ===
All tasks completed successfully
```

#### Custom Integration

For custom orchestration scripts, you can integrate learnings analysis manually:

```python
#!/usr/bin/env -S uv run
from flow import AgentInfo, Flow
from flow.lib.config import find_project_root
from flow.lib.flow_status import read_status
from flow.lib.learnings import get_improvable_files

def run_learnings_analyst(completed_flow_name: str) -> None:
    """Run learnings analysis after flow completion."""
    root = find_project_root()

    # Get flow metadata
    status = read_status(completed_flow_name)
    if status is None:
        return
    flow_type = status.metadata.flow_type or "unknown"

    # Get files the analyst can modify
    improvable = get_improvable_files(root)
    improvable_list = "\n".join(f"- {f}" for f in improvable)

    # Create analyst flow
    analyst_flow = Flow(
        f"learnings-{completed_flow_name}",
        config_path=".flow/learnings/analyst.yaml",
        reset=True,
    )

    # Run the analyst
    spawn_result = analyst_flow.spawn(
        agent_type="learnings-analyst",
        agent=AgentInfo(step=1, role="analyst"),
        input=f"""Analyze: {completed_flow_name}

## Flow Type
{flow_type}

## Files You May Propose Changes To
{improvable_list}
""",
    )

    results = analyst_flow.await_all([spawn_result])
    print(f"Analysis {'completed' if results[0].exit_reason == 'completed' else 'failed'}")

# Main implementation flow
flow = Flow("impl-feature", flow_type="implement", reset=True)

# ... run implementation steps ...

flow.set_status("completed")

# Optional: run learnings analysis
import os
if os.environ.get("RUN_LEARNINGS_ANALYSIS"):
    run_learnings_analyst(flow.name)
```

#### Integration Points

1. **After successful completion** - Most common, analyze what worked
2. **After failure** - Understand what went wrong
3. **Periodically** - Run aggregator on accumulated learnings

## Architecture

### Storage Layout

```
.flow/
  {flow_type}/
    learnings.jsonl          # Learning entries (JSONL, append-only)
    learning_clusters.json   # Pattern clusters (JSON)
```

### Data Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                 Human Feedback + Agent Analysis                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  flow feedback ──┐                    ┌── flow artifacts        │
│                  │                    │                         │
│                  ▼                    ▼                         │
│         ┌───────────────────────────────────────┐               │
│         │     .flow/{type}/learnings.jsonl      │               │
│         └───────────────────────────────────────┘               │
│                            │                                    │
│                            ▼                                    │
│         ┌───────────────────────────────────────┐               │
│         │    learnings-aggregator (periodic)    │               │
│         └───────────────────────────────────────┘               │
│                            │                                    │
│                            ▼                                    │
│         ┌───────────────────────────────────────┐               │
│         │  .flow/{type}/learning_clusters.json  │               │
│         └───────────────────────────────────────┘               │
│                            │                                    │
│                            ▼                                    │
│                    Pull Requests with                           │
│                  Improvement Proposals                          │
└─────────────────────────────────────────────────────────────────┘
```

### Improvable Files

The learnings agents derive which files they can modify from your flow configs:

1. **Agent instruction files** - `agents.*.system_prompt` with `@` prefix
2. **Non-writable shared files** - Static documentation and guidelines

This is determined by `get_improvable_files()` in `flow/lib/learnings.py`.

## Command Reference

### `flow feedback`

Submit human findings as learning entries.

```bash
flow feedback --flow <name> --category <cat> --title <title> [OPTIONS]
```

| Option                | Required | Description                                      |
| --------------------- | -------- | ------------------------------------------------ |
| `--flow`, `-f`        | Yes      | Flow name                                        |
| `--category`, `-c`    | Yes      | smoke_test, code_review, bug, missed_requirement |
| `--title`, `-t`       | Yes      | Brief title                                      |
| `--severity`, `-s`    | No       | low, medium (default), high, critical            |
| `--description`, `-d` | No       | Detailed description (supports @file.md)         |
| `--step`              | No       | Step number                                      |
| `--file`, `-F`        | No       | Affected files (repeatable)                      |
| `--tag`               | No       | Tags for clustering (repeatable)                 |
| `--json`              | No       | JSON output                                      |

### `flow learnings`

View learnings from the database.

```bash
flow learnings [OPTIONS]
```

| Option              | Description                                                                  |
| ------------------- | ---------------------------------------------------------------------------- |
| `--flow-type`, `-t` | Flow type to query                                                           |
| `--flow`, `-f`      | Filter by flow name                                                          |
| `--source`, `-s`    | Filter: flow_analysis, human_feedback, aggregation                           |
| `--category`, `-C`  | Filter: pattern, anti_pattern, tooling, instruction, bug, missed_requirement |
| `--clusters`        | Show clusters instead of entries                                             |
| `--all-types`       | Query across all flow types                                                  |
| `--limit`, `-n`     | Max entries (default: 20)                                                    |
| `--json`            | JSON output                                                                  |

### `flow learnings scaffold`

Set up the learnings system in your project.

```bash
flow learnings scaffold [--dry-run] [--overwrite]
```

| Option        | Description                    |
| ------------- | ------------------------------ |
| `--dry-run`   | Preview without creating files |
| `--overwrite` | Replace existing files         |

### `flow artifacts`

Get structured execution data for analysis.

```bash
flow artifacts --flow <name> [OPTIONS]
```

| Option                  | Description               |
| ----------------------- | ------------------------- |
| `--flow`, `-f`          | Flow name (required)      |
| `--format`              | json (default) or summary |
| `--include-transcripts` | Include transcript paths  |

## Customization

### Agent Prompts

After scaffolding, customize the agent prompts in `.flow/learnings/agents/`:

- `learnings-analyst.md` - What to look for, how to analyze
- `learnings-aggregator.md` - How to group patterns, what thresholds

### Flow Configurations

Adjust `.flow/learnings/*.yaml` for:

- Model selection (opus for thorough analysis, sonnet for speed)
- Timeout settings
- Agent interface (claude_code_cli or opencode)

### Pattern Detection Thresholds

In `flow/lib/learnings.py`, the `find_patterns()` function uses `min_occurrences=3`. You can adjust this by calling the function directly:

```python
from flow.lib.learnings import find_patterns

# More sensitive pattern detection
patterns = find_patterns("implement", min_occurrences=2)

# Less sensitive (requires more evidence)
patterns = find_patterns("implement", min_occurrences=5)
```

### Adding Custom Categories

If your project needs additional categories, extend the types in `flow/types.py`:

```python
class LearningCategory(str, Enum):
    PATTERN = "pattern"
    ANTI_PATTERN = "anti_pattern"
    # Add custom categories
    PERFORMANCE = "performance"
    ACCESSIBILITY = "accessibility"
```

## Related Documentation

- [USAGE_OVERVIEW.md](USAGE_OVERVIEW.md) - General Flow CLI usage
- [flow/docs/cli_docs.md](flow/docs/cli_docs.md) - CLI command reference
- [flow/docs/api_docs.md](flow/docs/api_docs.md) - Python API reference
